---
title: "Using News and Market Data to Predict Stock Movements"
course: "DATS 6101 Introduction to Data Science"
authors: "Mary Gibbs and Peter Riley"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

----------------
Project Overview
----------------

Research Question: Can a combinaton of news and market data predict whether a company's stock will increase or decrease within a day?

Data Source: https://www.kaggle.com/c/two-sigma-financial-news/data

----------------
Data Preparation
----------------

Packages
```{r}
library(bestglm)
library(car)
library(caret)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(MASS)
library(Metrics)
library(MLmetrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rattle)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
```

Clean Data
```{r}
#read in data 
df <- read.csv("data/data.csv") 
#remove X column
df$X <- NULL 
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
head(df)
```

Multicollinearity
```{r}
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot 
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfLogModel <- df[, c(4:5, 8:16)]
```

------
Models
------

Test/Train
```{r}
#get 80% train
train <- dfLogModel[1:11088, ]
#get 20% test
test <- dfLogModel[11089:13860, ]
```

Logistic Regression
-------------------

Full Logistic Regression Model 
```{r}
#full model 
fullModel <- glm(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, family = binomial(link = "logit"), data = train)

#full model summary
summary(fullModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15233, terrible reduction
#AIC is 15281

#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
```

Stepwise Logistic Regression Model
```{r}
#stepwise model
stepModel <- fullModel %>% stepAIC(trace = TRUE)

#stepwise model summary
summary(stepModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15237, terrible reduction
#AIC is 15271

#stepwise model VIF
vif(stepModel)
#VIF < 5, no multicollinearity issues
```

Stepwise Logistic Regression Model Prediction
```{r}
#stepwise model predicition
stepModelPrediction <- predict(stepModel, newdata = test, type = "response")
```

Stepwise Logistic Regression Model Hit Rate
```{r}
#stepwise model hit rate
stepModelPerLike <- ifelse(stepModelPrediction > 0.5, 1, 0)
stepModelHitRate <- mean(stepModelPerLike == test$binary_result)
stepModelHitRate
#stepwise model hit rate is 47.40%, terrible
```

Stepwise Logistic Regression Model ROC Curve
```{r}
#stepwise model ROC curve
stepModelROCpred <- prediction(stepModelPrediction, test$binary_result)
stepModelROCperf <- performance(stepModelROCpred, measure = "tpr", x.measure = "fpr")
plot(stepModelROCperf, xlab = "false positive rate", ylab = "true positive rate")
#stepwise model ROC curve is terrible

#stepwise model AUC
stepModelAUC <- performance(stepModelROCpred, measure = "auc")
stepModelAUC
#stepwise model AUC is 0.4922, terrible
```

Decision Tree
-------------

Decision Tree
```{r}
#decision tree
set.seed(1)
tree <- rpart(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, data = train, control = rpart.control(maxdepth = 6), method = "class")

#decision tree results
View(tree$cptable)
summary(tree)
#open and volume are important variables

#decision tree visualization
fancyRpartPlot(tree)
```

Decision Tree Prediction
```{r}
#decision tree prediction
treePrediction <- predict(tree, newdata = test, type = "class")
```

Decision Tree Hit Rate
```{r}
#decision tree true positive and true negative 
treePredictionTable <- table(test$binary_result, treePrediction) 
treePredictionTable

#decision tree confusion matrix
confusionMatrix(treePrediction, test$binary_result)
#decision tree hit rate is 49.06%, terrible
```
