#residuals - not normally distributed
#coefficients - total_expenditure p-value < 0.05, statistically significant relationship between education expenditure and 8th grade average math scores
#adjusted R-squared - 3.59% of the variance in 8th grade average math scores can be explained by education expenditure, poor model
#model assumptions
par(mfrow = c(2, 2))
plot(df_lm)
df_lm_res <- df_lm$residuals
describe(df_lm_res)
shapiro.test(df_lm_res)
#linearity - linearity, scatterplot shows linearity, residual vs fitted plot shows residuals skewed to left and spread around nearly horizontal line
#independence of residuals - independence of residuals, observations are independent
#normality of residuals - no normality of residuals, normal Q-Q plot shows that residuals follow straight line well, Shapiro-Wilk normality test shows p value < 0.05
#homoscedasticity of residuals - possible homoscedasticity of residuals, residual vs fitted plot shows residuals skewed to left and spread around slightly negatively sloped line
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(ResourceSelection)
library(pROC)
library(pscl)
library(ISLR)
library(dplyr)
library(ROCR)
library(bestglm)
library(openintro)
# Chunk 3
#read in data
data <- email
#check data
str(email)
#ensure variable classified correctly
data$to_multiple <- as.factor(data$to_multiple)
data$cc <- as.numeric(data$cc)
data$viagra <- as.factor(data$viagra)
data$format <- as.factor(data$format)
data$re_subj <- as.factor(data$re_subj)
data$exclaim_subj <- as.factor(data$exclaim_subj)
data$spam <- as.factor(data$spam)
#check data
str(data)
# Chunk 4
#logistic regression
#features: to_multiple, cc, attach, dollar, winner, inherit, viagra, password, format, re_subj, and exclaim_subj
#response: spam
spamlogit1 <- glm(spam ~ to_multiple + cc + attach + dollar + winner + inherit + viagra + password + format + re_subj + exclaim_subj, family = binomial(link = 'logit'), data = data)
#results
summary.glm(spamlogit1)
#to_multiple, attach, dollar, winner, inherit, password, format, and re_subj are statistically significant (p-value < 0.05), contribute to model
#null deviance is 2437.2 and residual deviance is 1931.5, not a great reduction
#AIC is 1955.5
#convert results to %
exp(coef(spamlogit1))
# Chunk 5
#get 80% train
train <- data[1:3137, ]
#get 20% test
test <- data[3138:3921,  ]
# Chunk 6
#logistic regression
#features: to_multiple, attach, dollar, winner, inherit, password, format, and re_subj
#response: spam
spamlogit2 <- glm(spam ~ to_multiple + attach + dollar + winner + inherit + password + format + re_subj, family = binomial(link = 'logit'), data = train)
#results
summary(spamlogit2)
#to_multiple, attach, winner, password, format, and re_subj are statistically significant (p-value < 0.05), contribute to model
#null deviance is 1729.6 and residual deviance is 1338.0, not a great reduction
#AIC is 1356, better than model 1
#convert results to %
exp(coef(spamlogit2))
# Chunk 7
#logistic regression
#features: to_multiple, attach, winner, password, format, and re_subj
#response: spam
spamlogit3 <- glm(spam ~ to_multiple + attach + winner + password + format + re_subj, family = binomial(link = 'logit'), data = train)
#results
summary(spamlogit3)
#to_multiple, attach, winner, password, format, and re_subj are statistically significant (p-value < 0.05), contribute to model
#null deviance is 1729.6 and residual deviance is 1343.6, not a great reduction
#AIC is 1357.6, better than model 1, worse than model 2
#convert results to %
exp(coef(spamlogit3))
# Chunk 8
#model 2 prediction
predict_spamlogit2 <- predict.glm(spamlogit2, test, type = 'response')
# Chunk 9
#model 2 hit rate
per_like_spamlogit2 <- ifelse(predict_spamlogit2 > 0.5, 1, 0)
hit_rate_spamlogit2 <- mean(per_like_spamlogit2 == test$spam)
hit_rate_spamlogit2
#model 2 hit rate is 85.20%, okay
#model 2 ROC curve
roc_spamlogit2 <- prediction(predict_spamlogit2, test$spam)
perf_roc_spamlogit2 <- performance(roc_spamlogit2, measure = 'tpr', x.measure = 'fpr')
plot(perf_roc_spamlogit2)
#model 2 ROC curve is okay
#model 2 AUC
perf_auc_spamlogit2 <- performance(roc_spamlogit2, measure = 'auc')
perf_auc_spamlogit2
#model 2 AUC is 0.76, okay
#model 2 is okay at predicting spam
# Chunk 1
library(bestglm)
library(car)
library(caret)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(Metrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
# Chunk 2
#read in data
df <- read.csv("data/data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
head(df)
# Chunk 3
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfLogModel <- df[, c(4:5, 8:16)]
# Chunk 4
#get 80% train
train <- dfLogModel[1:11088, ]
#get 20% test
test <- dfLogModel[11089:13860, ]
# Chunk 5
#full model
fullModel <- glm(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, family = binomial(link = "logit"), data = train)
#full model summary
summary(fullModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15233, terrible reduction
#AIC is 15281
#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
str(df)
setwd("~/Documents/GitHub/R/DATS 6101 Project 2")
# Chunk 1
library(bestglm)
library(car)
library(caret)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(Metrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
# Chunk 2
#read in data
df <- read.csv("data/data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
head(df)
# Chunk 3
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfLogModel <- df[, c(4:5, 8:16)]
# Chunk 4
#get 80% train
train <- dfLogModel[1:11088, ]
#get 20% test
test <- dfLogModel[11089:13860, ]
# Chunk 5
#full model
fullModel <- glm(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, family = binomial(link = "logit"), data = train)
#full model summary
summary(fullModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15233, terrible reduction
#AIC is 15281
#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
str(df)
#best model - commented out due to run time
colnames(train) <- c("volume", "open", "y", "sentimentNegative", "sentimentNeutral", "sentimentPositive", "urgency", "firstMentionSentence", "bodySize")
bestTrain <- train[, c("volume", "open", "sentimentNegative", "sentimentNeutral", "sentimentPositive", "urgency", "firstMentionSentence", "bodySize", "y")]
logres.bglm <- bestglm(Xy = bestTrain, family = binomial, IC = "AIC", method = "exhaustive")
logres.bglm$BestModels
#best model
bestModel <- glm(binary_result ~ volume + open + provider + sentimentNegative + firstMentionSentence, family = binomial(link = "logit"), data = train)
#best model summary
summary(bestModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15235, terrible reduction
#AIC is 15271
#best model VIF
vif(bestModel)
#VIF < 5, no multicollinearity issues
str(df)
#best model - commented out due to run time
colnames(train) <- c("volume", "open", "y", "sentimentNegative", "sentimentNeutral", "sentimentPositive", "firstMentionSentence", "bodySize")
#VIF < 5, no multicollinearity issues
str(bestTrain)
#best model - commented out due to run time
colnames(train) <- c("volume", "open", "y", "sentimentClass", "sentimentNegative", "sentimentNeutral", "sentimentPositive", "urgency", "firstMentionSentence", "bodySize")
bestTrain <- train[, c("volume", "open", "sentimentNegative", "sentimentNeutral", "sentimentPositive", "firstMentionSentence", "bodySize", "y")]
str(bestTrain)
#best model - commented out due to run time
bestTrain <- train %>% select(-"binary_result", "binary_result")
View(train)
View(train)
# Chunk 1
library(bestglm)
library(car)
library(caret)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(Metrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
# Chunk 2
#read in data
df <- read.csv("data/data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
head(df)
# Chunk 3
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfLogModel <- df[, c(4:5, 8:16)]
# Chunk 4
#get 80% train
train <- dfLogModel[1:11088, ]
#get 20% test
test <- dfLogModel[11089:13860, ]
# Chunk 5
#full model
fullModel <- glm(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, family = binomial(link = "logit"), data = train)
#full model summary
summary(fullModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15233, terrible reduction
#AIC is 15281
#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
#best model - commented out due to run time
train %>% select(-binary_result, binary_result)
View(train)
View(train)
#best model - commented out due to run time
bestTrain <- test[c(1:2, 4:11, 3)]
str(bestTrain)
names(bestTrain) <- c("y", "binary_result")
str(bestTrain)
# Chunk 1
library(bestglm)
library(car)
library(caret)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(Metrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
# Chunk 2
#read in data
df <- read.csv("data/data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
head(df)
# Chunk 3
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfLogModel <- df[, c(4:5, 8:16)]
# Chunk 4
#get 80% train
train <- dfLogModel[1:11088, ]
#get 20% test
test <- dfLogModel[11089:13860, ]
# Chunk 5
#full model
fullModel <- glm(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, family = binomial(link = "logit"), data = train)
#full model summary
summary(fullModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15233, terrible reduction
#AIC is 15281
#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
#best model - commented out due to run time
bestTrain <- test[c(1:2, 4:11, 3)]
colnames(bestTrain)[colnames(bestTrain) == "binary_result"] <- "y"
str(bestTrain)
#best model - commented out due to run time
bestTrain <- test[c(1:2, 5:11, 3)]
colnames(bestTrain)[colnames(bestTrain) == "binary_result"] <- "y"
str(bestTrain)
#best model - commented out due to run time
bestTrain <- test[c(1:2, 6:11, 3)]
colnames(bestTrain)[colnames(bestTrain) == "binary_result"] <- "y"
str(bestTrain)
logres.bglm <- bestglm(Xy = bestTrain, family = binomial, IC = "AIC", method = "exhaustive")
logres.bglm$BestModels
#best model
bestModel <- glm(binary_result ~ volume + open + firstMentionSentence, family = binomial(link = "logit"), data = train)
#best model summary
summary(bestModel)
library(MASS)
#stepwise model
stepmodel <- fullModel %>% stepAIC(trace = TRUE)
#stepwise model summary
summary(stepmodel)
#step model VIF
vif(stepmodel)
#best model predicition
stepModelPrediction <- predict(stepModel, newdata = test, type = "response")
setwd("~/Documents/GitHub/R/DATS 6101 Project 2")
# Chunk 1
library(bestglm)
library(car)
library(caret)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(MASS)
library(Metrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
# Chunk 2
#read in data
df <- read.csv("data/data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
head(df)
# Chunk 3
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfLogModel <- df[, c(4:5, 8:16)]
# Chunk 4
#get 80% train
train <- dfLogModel[1:11088, ]
#get 20% test
test <- dfLogModel[11089:13860, ]
# Chunk 5
#full model
fullModel <- glm(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, family = binomial(link = "logit"), data = train)
#full model summary
summary(fullModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15233, terrible reduction
#AIC is 15281
#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
# Chunk 6
#stepwise model
stepmodel <- fullModel %>% stepAIC(trace = TRUE)
#stepwise model summary
summary(stepmodel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15237, terrible reduction
#AIC is 15271
#step model VIF
vif(stepmodel)
#VIF < 5, no multicollinearity issues
# Chunk 7
#best model predicition
stepModelPrediction <- predict(stepModel, newdata = test, type = "response")
# Chunk 8
#best model hit rate
stepModelPerLike <- ifelse(stepModelPrediction > 0.5, 1, 0)
stepModelHitRate <- mean(stepModelPerLike == test$binary_result)
stepModelHitRate
#best model hit rate is 47.44%, terrible
# Chunk 9
#best model ROC curve
bestModelROCpred <- prediction(bestModelPrediction, test$binary_result)
bestModelROCperf <- performance(bestModelROCpred, measure = "tpr", x.measure = "fpr")
plot(bestModelROCperf, xlab = "false positive rate", ylab = "true positive rate")
#best ROC curve is terrible
#best model AUC
bestModelAUC <- performance(bestModelROCpred, measure = "auc")
bestModelAUC
#model 2 AUC is 0.4967, terrible
# Chunk 10
#build model
tree.results <- rpart(binary_result ~ time+sentimentClass+sentimentNegative+sentimentNeutral+sentimentPositive+bodySize+wordCount, data = training, control= rpart.control(cp=.005), method = "class")
plot(tree.results, uniform=TRUE, margin=.05)
text(tree.results)
summary(tree.results)
# Draw the decision tree using fancyRpart and text tree
fancyRpartPlot(tree.results)
plot(tree.results, uniform=TRUE, margin=.05)
text(tree.results)
#make predictions
tree_pred<- predict(tree.results, newdata=dfTest, type="class")
#misclassification table (the (0, 0) (1,1) diagonals are the correct classifications)
with(dfTest, table(tree_pred, binary_result))
#using the table, we'll find the error
error_value<-  (1021+629)/ 2771
error_value
#best model predicition
stepModelPrediction <- predict(stepModel, newdata = test, type = "response")
#stepwise model
stepModel <- fullModel %>% stepAIC(trace = TRUE)
#stepwise model summary
summary(stepmodel)
#best model predicition
stepModelPrediction <- predict(stepModel, newdata = test, type = "response")
stepModelHitRate
#best model hit rate
stepModelPerLike <- ifelse(stepModelPrediction > 0.5, 1, 0)
stepModelHitRate <- mean(stepModelPerLike == test$binary_result)
stepModelHitRate
#stepwise model ROC curve
stepModelROCpred <- prediction(stepModelPrediction, test$binary_result)
stepModelROCperf <- performance(stepModelROCpred, measure = "tpr", x.measure = "fpr")
plot(stepModelROCperf, xlab = "false positive rate", ylab = "true positive rate")
