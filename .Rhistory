#null deviance is 15370 and residual deviance is 15282, terrible reduction
#AIC is 15330
#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
# Chunk 6
#stepwise model
stepModel <- fullModel %>% stepAIC(trace = TRUE)
#stepwise model summary
summary(stepModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15237, terrible reduction
#AIC is 15271
#stepwise model VIF
vif(stepModel)
#VIF < 5, no multicollinearity issues
# Chunk 7
#stepwise model predicition
stepModelPrediction <- predict(stepModel, newdata = test, type = "response")
# Chunk 8
#stepwise model accuracy
stepModelPerLike <- ifelse(stepModelPrediction > 0.5, 1, 0)
stepModelAccuracy <- mean(stepModelPerLike == test$binary_result)
stepModelAccuracy
#stepwise model accuracy is 47.40%, terrible
# Chunk 9
#stepwise model ROC curve
stepModelROCpred <- prediction(stepModelPrediction, test$binary_result)
stepModelROCperf <- performance(stepModelROCpred, measure = "tpr", x.measure = "fpr")
plot(stepModelROCperf, xlab = "false positive rate", ylab = "true positive rate")
#stepwise model ROC curve is terrible
#stepwise model AUC
stepModelAUC <- performance(stepModelROCpred, measure = "auc")
stepModelAUC
#stepwise model AUC is 0.4922, terrible
# Chunk 10
#decision tree
set.seed(1)
tree <- rpart(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, data = train, control = rpart.control(maxdepth = 6), method = "class")
#decision tree results
View(tree$cptable)
summary(tree)
#open and volume are important variables
#decision tree visualization
fancyRpartPlot(tree)
# Chunk 11
#decision tree prediction
treePrediction <- predict(tree, newdata = test, type = "class")
# Chunk 12
#decision tree true positive and true negative
treePredictionTable <- table(test$binary_result, treePrediction)
treePredictionTable
#decision tree confusion matrix
confusionMatrix(treePrediction, test$binary_result)
#decision tree accuracy is 49.06%, terrible
#full model summary
summary(fullModel)
#full model VIF
vif(fullModel)
#stepwise model summary
summary(stepModel)
#stepwise model VIF
vif(stepModel)
stepModelAccuracy
#stepwise model predicition
stepModelPrediction <- predict(stepModel, newdata = test, type = "response")
# Chunk 1
library(bestglm)
library(car)
library(caret)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(MASS)
library(Metrics)
library(MLmetrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rattle)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
# Chunk 2
#read in data
df <- read.csv("data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
# Chunk 3
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfModel <- df[, c(4:5, 8:16)]
# Chunk 4
#random seed
set.seed(1)
#shuffle data
n <- nrow(dfModel)
shuffle <- dfModel[sample(n), ]
#get 80% train
trainIndices <- 1:round(0.80*n)
train <-shuffle[trainIndices, ]
#get 20% test
testIndices <- (round(0.80*n) + 1):n
test <- shuffle[testIndices, ]
# Chunk 5
#full model
fullModel <- glm(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, family = binomial(link = "logit"), data = train)
#full model summary
summary(fullModel)
#volume is statistically significant (p-value < 0.05), contributes to model
#null deviance is 15363 and residual deviance is 15270, terrible reduction
#AIC is 15316
#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
# Chunk 6
#stepwise model
stepModel <- fullModel %>% stepAIC(trace = TRUE)
#stepwise model summary
summary(stepModel)
#volume isstatistically significant (p-value < 0.05), contributes to model
#null deviance is 15363 and residual deviance is 15275, terrible reduction
#AIC is 15305, barely better than full model
#stepwise model VIF
vif(stepModel)
#VIF < 5, no multicollinearity issues
# Chunk 7
#stepwise model predicition
stepModelPrediction <- predict(stepModel, newdata = test, type = "response")
# Chunk 8
#stepwise model accuracy
stepModelPerLike <- ifelse(stepModelPrediction > 0.5, 1, 0)
stepModelAccuracy <- mean(stepModelPerLike == test$binary_result)
stepModelAccuracy
#stepwise model accuracy is 47.40%, terrible
# Chunk 9
#stepwise model ROC curve
stepModelROCpred <- prediction(stepModelPrediction, test$binary_result)
stepModelROCperf <- performance(stepModelROCpred, measure = "tpr", x.measure = "fpr")
plot(stepModelROCperf, xlab = "false positive rate", ylab = "true positive rate")
#stepwise model ROC curve is terrible
#stepwise model AUC
stepModelAUC <- performance(stepModelROCpred, measure = "auc")
stepModelAUC
#stepwise model AUC is 0.4922, terrible
# Chunk 10
#decision tree
set.seed(1)
tree <- rpart(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, data = train, control = rpart.control(maxdepth = 6), method = "class")
#decision tree results
View(tree$cptable)
summary(tree)
#open and volume are important variables
#decision tree visualization
fancyRpartPlot(tree)
# Chunk 11
#decision tree prediction
treePrediction <- predict(tree, newdata = test, type = "class")
# Chunk 12
#decision tree true positive and true negative
treePredictionTable <- table(test$binary_result, treePrediction)
treePredictionTable
#decision tree confusion matrix
confusionMatrix(treePrediction, test$binary_result)
#decision tree accuracy is 49.06%, terrible
# Chunk 1
library(bestglm)
library(car)
library(caret)
library(caTools)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(MASS)
library(Metrics)
library(MLmetrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rattle)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
# Chunk 2
#read in data
df <- read.csv("data/data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
# Chunk 3
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfModel <- df[, c(4:5, 8:16)]
# Chunk 4
#get 80% train
train <- dfModel[1:11088, ]
#get 20% test
test <- dfModel[11089:13860, ]
# Chunk 5
#full model
fullModel <- glm(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, family = binomial(link = "logit"), data = train)
#full model summary
summary(fullModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15233, terrible reduction
#AIC is 15281
#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
# Chunk 6
#stepwise model
stepModel <- fullModel %>% stepAIC(trace = TRUE)
#stepwise model summary
summary(stepModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15237, terrible reduction
#AIC is 15271
#stepwise model VIF
vif(stepModel)
#VIF < 5, no multicollinearity issues
# Chunk 7
#stepwise model predicition
stepModelPrediction <- predict(stepModel, newdata = test, type = "response")
# Chunk 8
#stepwise model accuracy
stepModelPerLike <- ifelse(stepModelPrediction > 0.5, 1, 0)
stepModelAccuracy <- mean(stepModelPerLike == test$binary_result)
stepModelAccuracy
#stepwise model accuracy is 47.40%, terrible
# Chunk 9
#stepwise model ROC curve
stepModelROCpred <- prediction(stepModelPrediction, test$binary_result)
stepModelROCperf <- performance(stepModelROCpred, measure = "tpr", x.measure = "fpr")
plot(stepModelROCperf, xlab = "false positive rate", ylab = "true positive rate")
#stepwise model ROC curve is terrible
#stepwise model AUC
stepModelAUC <- performance(stepModelROCpred, measure = "auc")
stepModelAUC
#stepwise model AUC is 0.4922, terrible
# Chunk 10
#decision tree
set.seed(1)
tree <- rpart(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, data = train, control = rpart.control(maxdepth = 6), method = "class")
#decision tree results
View(tree$cptable)
summary(tree)
#open and volume are important variables
#decision tree visualization
fancyRpartPlot(tree)
# Chunk 11
#decision tree prediction
treePrediction <- predict(tree, newdata = test, type = "class")
# Chunk 12
#decision tree true positive and true negative
treePredictionTable <- table(test$binary_result, treePrediction)
treePredictionTable
#decision tree confusion matrix
confusionMatrix(treePrediction, test$binary_result)
#decision tree accuracy is 49.06%, terrible
setwd("~/Documents/GitHub/R/DATS 6101 Project 2")
# Chunk 1
library(bestglm)
library(car)
library(caret)
library(caTools)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(MASS)
library(Metrics)
library(MLmetrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rattle)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
# Chunk 2
#read in data
df <- read.csv("data/data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
# Chunk 3
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfModel <- df[, c(4:5, 8:16)]
# Chunk 4
#get 80% train
train <- dfModel[1:11088, ]
#get 20% test
test <- dfModel[11089:13860, ]
# Chunk 5
#full model
fullModel <- glm(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, family = binomial(link = "logit"), data = train)
#full model summary
summary(fullModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15233, terrible reduction
#AIC is 15281
#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
# Chunk 6
#stepwise model
stepModel <- fullModel %>% stepAIC(trace = TRUE)
#stepwise model summary
summary(stepModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15237, terrible reduction
#AIC is 15271
#stepwise model VIF
vif(stepModel)
#VIF < 5, no multicollinearity issues
# Chunk 7
#stepwise model predicition
stepModelPrediction <- predict(stepModel, newdata = test, type = "response")
# Chunk 8
#stepwise model accuracy
stepModelPerLike <- ifelse(stepModelPrediction > 0.5, 1, 0)
stepModelAccuracy <- mean(stepModelPerLike == test$binary_result)
stepModelAccuracy
#stepwise model accuracy is 47.40%, terrible
# Chunk 9
#stepwise model ROC curve
stepModelROCpred <- prediction(stepModelPrediction, test$binary_result)
stepModelROCperf <- performance(stepModelROCpred, measure = "tpr", x.measure = "fpr")
plot(stepModelROCperf, xlab = "false positive rate", ylab = "true positive rate")
#stepwise model ROC curve is terrible
#stepwise model AUC
stepModelAUC <- performance(stepModelROCpred, measure = "auc")
stepModelAUC
#stepwise model AUC is 0.4922, terrible
# Chunk 10
#decision tree
set.seed(1)
tree <- rpart(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, data = train, control = rpart.control(maxdepth = 6), method = "class")
#decision tree results
View(tree$cptable)
summary(tree)
#open and volume are important variables
#decision tree visualization
fancyRpartPlot(tree)
# Chunk 11
#decision tree prediction
treePrediction <- predict(tree, newdata = test, type = "class")
# Chunk 12
#decision tree true positive and true negative
treePredictionTable <- table(test$binary_result, treePrediction)
treePredictionTable
#decision tree confusion matrix
confusionMatrix(treePrediction, test$binary_result)
#decision tree accuracy is 49.06%, terrible
# Chunk 1
library(bestglm)
library(car)
library(caret)
library(caTools)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(MASS)
library(Metrics)
library(MLmetrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rattle)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
# Chunk 2
#read in data
df <- read.csv("data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
# Chunk 3
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfModel <- df[, c(4:5, 8:16)]
# Chunk 4
#get 80% train
train <- dfModel[1:11088, ]
#get 20% test
test <- dfModel[11089:13860, ]
# Chunk 5
#full model
fullModel <- glm(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, family = binomial(link = "logit"), data = train)
#full model summary
summary(fullModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15233, terrible reduction
#AIC is 15281
#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
# Chunk 6
#stepwise model
stepModel <- fullModel %>% stepAIC(trace = TRUE)
#stepwise model summary
summary(stepModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15237, terrible reduction
#AIC is 15271
#stepwise model VIF
vif(stepModel)
#VIF < 5, no multicollinearity issues
# Chunk 7
#stepwise model predicition
stepModelPrediction <- predict(stepModel, newdata = test, type = "response")
# Chunk 8
#stepwise model accuracy
stepModelPerLike <- ifelse(stepModelPrediction > 0.5, 1, 0)
stepModelAccuracy <- mean(stepModelPerLike == test$binary_result)
stepModelAccuracy
#stepwise model accuracy is 47.40%, terrible
# Chunk 9
#stepwise model ROC curve
stepModelROCpred <- prediction(stepModelPrediction, test$binary_result)
stepModelROCperf <- performance(stepModelROCpred, measure = "tpr", x.measure = "fpr")
plot(stepModelROCperf, xlab = "false positive rate", ylab = "true positive rate")
#stepwise model ROC curve is terrible
#stepwise model AUC
stepModelAUC <- performance(stepModelROCpred, measure = "auc")
stepModelAUC
#stepwise model AUC is 0.4922, terrible
# Chunk 10
#decision tree
set.seed(1)
tree <- rpart(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, data = train, control = rpart.control(maxdepth = 6), method = "class")
#decision tree results
View(tree$cptable)
summary(tree)
#open and volume are important variables
#decision tree visualization
fancyRpartPlot(tree)
# Chunk 11
#decision tree prediction
treePrediction <- predict(tree, newdata = test, type = "class")
# Chunk 12
#decision tree true positive and true negative
treePredictionTable <- table(test$binary_result, treePrediction)
treePredictionTable
#decision tree confusion matrix
confusionMatrix(treePrediction, test$binary_result)
#decision tree accuracy is 49.06%, terrible
#full model summary
summary(fullModel)
setwd("~/Documents/GitHub/R/DATS 6101 Project 2")
